[JOB_PARAMS]
buff_capacity = [1e6]
num_episodes = [5000]
batch_size = [64]
gamma = [0.95]

eps_start = [1]
eps_end = [0.05]
eps_decay = [4000]

target_update = [3]
update_steps = [8]
hidden_size = [128]
ddqn = [0]

; Optimizer
lr = [1e-4]
opt = ['adam']
; Environment Specific
;K = [1, 20, 50]
K = [1]
grid_size = [15]
pct = [0.75]
pcf = [0.5]
max_steps = [100]
rnd_start = [1]
start_state_exclude_rooms = ["4 5 6 7 8 9"]

; Validation
; Number of rollouts to estimate the validation performance
val_rollouts = [30]
; Validate every these many episodes
val_episode = [50] 
; 1 if evaluation with unseen start states needed
unseen = [0]

; Others
gpu = [1]
seed = [101]

[SCRIPT_PARAMS]
; Identifier for the run script instance, useful for identifying
; all jobs belonging to the same hyperparameter sweep
run_id = "train.debug.v4.4"
#run_id = "test_run"

; The training file to run the job
run_file = "train.py"

; If True, all checkpoint sub-dirs across all jobs will
; have a common prefix == run_id and an integer suffix indicating the
; index of the job in the hyperparameter sweep.
; If False, a randomly generated sub-dir name is used for all jobs.
common_save_subdir = False

; Whether to wait i.e. block while a job is running. Use wait -> True
; for debugging, else keep it False.
wait = True 

; Whether to pipe stdout to cmd or to a log file
; Set to false if using debug jobs
redirect_stdout = False 

; Slurm queue type, either "debug", "short", "long" or "noslurm"
; where the first three are SLURM queue types and "noslurm" just
; runs the command directly without using SLURM. Use "noslurm"
; when in an interactive slurm debug queue.
slurm_queue_type = "noslurm"

; If visdom_env_name is None, automatically generated name is used.
; For single debug jobs, a custom name is recommended in which
; case visdom_env_name should be set to a string value.
; For hyperparameter sweeps, it is advisabvle to use automatically
; generated visdom environment names.
visdom_env_name = None

; Do a dry run of run.py without launching jobs
dry_run = False 

; Whether to run job as "CUDA_LAUNCH_BLOCKING=1 python ..."
cuda_launch_blocking = False

; Custom time limit for job
time_limit = 0

; slurm's --exclude node-list
exclude_nodes = ""