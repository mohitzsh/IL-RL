[JOB_PARAMS]
buff_capacity = [1e10]
num_episodes = [1e5]
batch_size = [512]
gamma = [0.95]

eps_start = [1]
eps_end = [0.05]
eps_decay = [200]

target_update = [10]
update_steps = [4]
hidden_size = [128]
ddqn = [0]

; Optimizer
lr = [1e-3]
opt = ['adam']

; Environment Specific
K = [1]
grid_size = [10]
pct = [0.75]
pcf = [0.5]
max_steps = [100]
rnd_start = [0]
val_rollouts = [10]
val_episode = [1000]

;others
seed = [101]
gpu = [1]
[SCRIPT_PARAMS]
; Identifier for the run script instance, useful for identifying
; all jobs belonging to the same hyperparameter sweep
run_id = "main"
#run_id = "test_run"

; The training file to run the job
run_file = "run_train.py"

; If True, all checkpoint sub-dirs across all jobs will
; have a common prefix == run_id and an integer suffix indicating the
; index of the job in the hyperparameter sweep.
; If False, a randomly generated sub-dir name is used for all jobs.
common_save_subdir = False

; Whether to wait i.e. block while a job is running. Use wait -> True
; for debugging, else keep it False.
wait = False 

; Whether to pipe stdout to cmd or to a log file
; Set to false if using debug jobs
redirect_stdout = True 

; Slurm queue type, either "debug", "short", "long" or "noslurm"
; where the first three are SLURM queue types and "noslurm" just
; runs the command directly without using SLURM. Use "noslurm"
; when in an interactive slurm debug queue.
slurm_queue_type = "debug"

; If visdom_env_name is None, automatically generated name is used.
; For single debug jobs, a custom name is recommended in which
; case visdom_env_name should be set to a string value.
; For hyperparameter sweeps, it is advisabvle to use automatically
; generated visdom environment names.
visdom_env_name = None

; Do a dry run of run.py without launching jobs
dry_run = False 

; Whether to run job as "CUDA_LAUNCH_BLOCKING=1 python ..."
cuda_launch_blocking = False

; Custom time limit for job
time_limit = 0

; slurm's --exclude node-list
exclude_nodes = "smith,hal"