[JOB_PARAMS]
gamma = [0.99]
hidden_size = [128]
env = ['cluttered']
;env = ['empty']
grid_size = [12]
nb_objects = [12]
obj_size = [2]
max_episodes = [15000]
max_steps = [100]
;pg_lr = [1e-4, 5e-4, 1e-3, 5e-3]
pg_lr = [1e-3, 1e-4]
;pg_lr = [1e-3]
im_lr = [1e-3]
; beta_im schedule
; Linear Decay, start with {beta_im_start},
; decay to {beta_im_end} at episode {\beta_im_ep}, and
; stay there till end
beta_im_ep = [15000]
;beta_im_start = [1, 0.1, 0.01]
beta_im_start = [1]
beta_im_end = [1]


beta_rl = [1]

imitate = [1]
reinforce = [1]
sampling_zone = ["4"]
state_encoding = ["one-hot"]
reinforce_mode = ["hier"]
reinforce_norm = ["sum"]
weigh_class = [0]
n_policies = [5]
#sampling_zone = []
batch_size = [16]
nb_epochs = [100]
gpu = [1]
val_frequency = [20]
log_dir = ['save/logs/PG']

[SCRIPT_PARAMS]
; Identifier for the run script instance, useful for identifying
; all jobs belonging to the same hyperparameter sweep
;run_id = "debug.v.5.1.5.1"

run_id = "algo1-il-rl.v.7.4"

; The training file to run the job
run_file = "imitate_and_reinforce.py"

; If True, all checkpoint sub-dirs across all jobs will
; have a common prefix == run_id and an integer suffix indicating the
; index of the job in the hyperparameter sweep.
; If False, a randomly generated sub-dir name is used for all jobs.
common_save_subdir = False

; Whether to wait i.e. block while a job is running. Use wait -> True
; for debugging, else keep it False.

#wait = False 
#redirect_stdout = True 
#slurm_queue_type = "debug"

wait = False 
redirect_stdout = True 
slurm_queue_type = "short"

#wait = False 
#redirect_stdout = True 
#slurm_queue_type = "long"

#wait = True 
#redirect_stdout = False 
#slurm_queue_type = "noslurm"



; If visdom_env_name is None, automatically generated name is used.
; For single debug jobs, a custom name is recommended in which
; case visdom_env_name should be set to a string value.
; For hyperparameter sweeps, it is advisabvle to use automatically
; generated visdom environment names.
visdom_env_name = None

; Do a dry run of run.py without launching jobs
dry_run = False 

; Whether to run job as "CUDA_LAUNCH_BLOCKING=1 python ..."
cuda_launch_blocking = False

; Custom time limit for job
time_limit = 0

; slurm's --exclude node-list
exclude_nodes = "alexa,ash"